# Parametric kernel low-rank approximations using tensor train decomposition
This repository contains Python code for computing low-rank approximations of parametric kernel matrices with the tensor train decomposition. It accompanies the paper
> Khan, A., & Saibaba, A. K. (2024). Parametric kernel low-rank approximations using tensor train decomposition. Submitted. [arXiv preprint](https://arxiv.org/abs/2406.06344).

The results generated in the [folder](https://github.com/awkhan3/ParametricTensorTrainKernel/tree/main/experiments_out) were generated by this code on the Hazel Cluster at North Carolina State University.

## Requirements
The [Python](python/) code requires the following packages to generate the tables.
1. [numpy](https://github.com/numpy/numpy)
2. [tensorly](https://github.com/scipy/scipy)
3. [pytorch](https://github.com/pytorch/pytorch)
4. [numba](https://github.com/numba/numba)
5. [scikit-learn](https://github.com/scikit-learn/scikit-learn)
6. [tntorch(forked)](https://github.com/awkhan3/tntorch)
7. [py-markdown-table](https://pypi.org/project/py-markdown-table/)
8. [RP-Cholesky](https://github.com/eepperly/Randomly-Pivoted-Cholesky) (note, we have a local copy of the RP-Cholesky method for reproducbility)

## License
To use these codes in your research, please see the [License](LICENSE). If you find our code useful, please consider citing our paper.
```bibtex
@article{khan2024parametric,
  title={Parametric kernel low-rank approximations using tensor train decomposition},
  author={Khan, Abraham and Saibaba, Arvind K},
  journal={arXiv preprint arXiv:2406.06344},
  year={2024}
}
```
## Python Enviroment Setup
1. `chmod +x setup_env.sh`
2. `./setup_env`

## Generating Tables
1. `chmod +x run_experiments.sh`
2. `./run_experiments.sh`

## Experiments
1. [experiment_1](https://github.com/awkhan3/ParametricTensorTrainKernel/blob/main/experiments/experiment_1.py) compares our TTK (Tensor Train Kernel) method with thin SVD.
2. [experiment_2](https://github.com/awkhan3/ParametricTensorTrainKernel/blob/main/experiments/experiment_2.py) compares our PTTK method with Adaptive Cross Approximation (ACA).
3. [experiment_3](https://github.com/awkhan3/ParametricTensorTrainKernel/blob/main/experiments/experiment_3.py) performs PTTK-Global-1 and PTTK-Global-2 on symmetric kernel matrices.
4. [experiment_4](https://github.com/awkhan3/ParametricTensorTrainKernel/blob/main/experiments/experiment_4.py) compares our PTTK-Global-1 and PTTK-Global-2 method with [RP-Cholesky](https://github.com/eepperly/Randomly-Pivoted-Cholesky) and Nystrom on a symmetric positve semi-definite kernel matrix.
5. [experiment_5](https://github.com/awkhan3/ParametricTensorTrainKernel/blob/main/experiments/experiment_5.py) compares our PTTK method with [Tucker applied to parametric kernel matrices](https://link.springer.com/article/10.1007/s10444-022-09979-7) in three spatial dimensions, can easily be edited to two spatial dimensons by changing the dimension parameter.

## Funding
This work was funded by the National Science Foundation through the awards DMS-1845406, DMS-1821149, and 
DMS-2026830.


 
